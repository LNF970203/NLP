{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94f5436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "import en_core_web_sm\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf716ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d5e48a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = ['I like to play computer games', 'I like to run']\n",
    "list2 = ['I like playing games', 'I like walking']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cc591dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self,list1,list2):\n",
    "        self.list1 = list1\n",
    "        self.list2 = list2\n",
    "        if (len(list1) != len(list2)):\n",
    "            print('Length of 2 inputs must be equal')\n",
    "        \n",
    "    def __preprocessing(self,text):\n",
    "        \"\"\" - Preprocessing the text\n",
    "            - Planning to use the tfidf vectorizer, therefore preprocessing and removing stops words are essential\n",
    "        \"\"\"\n",
    "        text = text.lower()\n",
    "        token_words = [token.text for token in nlp(text) if not token.is_stop]\n",
    "        cleaned_sentences = ' '.join(token_words)\n",
    "        return cleaned_sentences\n",
    "    \n",
    "    def precision(self, number_of_questions_to_be_answered ,threshold = 0.5, preprocessed = False):\n",
    "        \"\"\"\n",
    "        return 3 outputs as precision, recall and Fmeasure\n",
    "        number_of_questions_to_be_answered - Questions to be answered in future - {int}\n",
    "        threshold - threshold value to compare cosine similarity {between 0 to 1, float}\n",
    "        preprocessed - if true stop words are removed. by default flase\n",
    "        \"\"\"\n",
    "        self.__answer_count = 0\n",
    "        if preprocessed:\n",
    "            self.__prelist1 = list(map(self.__preprocessing, self.list1))\n",
    "            self.__prelist2 = list(map(self.__preprocessing, self.list2))\n",
    "            \n",
    "            for i,j in zip(self.__prelist1, self.__prelist2):\n",
    "                s1 = nlp(i)\n",
    "                s2 = nlp(j)\n",
    "                score = s1.similarity(s2)\n",
    "                if (score >= threshold):\n",
    "                    self.__answer_count += 1\n",
    "        else:\n",
    "            for i,j in zip(self.list1,self.list2):\n",
    "                w1 = nlp(i)\n",
    "                w2 = nlp(j)\n",
    "                score1 = w1.similarity(w2)\n",
    "                if (score1 >= threshold):\n",
    "                    self.__answer_count += 1\n",
    "        \n",
    "        self.__precision = self.__answer_count/len(self.list1)\n",
    "        self.__recall = self.__answer_count/number_of_questions_to_be_answered\n",
    "        self.__Fmeasure = 2*self.__precision*self.__recall / (self.__precision+self.__recall)\n",
    "        return self.__precision, self.__recall, self.__Fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa15337e",
   "metadata": {},
   "outputs": [],
   "source": [
    "e2 = Evaluator(list1,list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2d0ddcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-13bea8abf9d8>:39: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  score1 = w1.similarity(w2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0, 0.4, 0.5714285714285715)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e2.precision(5, preprocessed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf9a739",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcc666c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
